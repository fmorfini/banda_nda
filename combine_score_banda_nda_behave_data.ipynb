{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06429bd-60ec-4bda-95b4-09c1c4511403",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BANDA behavioral, clinical, NIH task data from NDA\n",
    "#### francesca morfini; susan whitfield-gabrieli lab; Northeastern University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41802fef-8b71-4b4a-9998-58a0ccb36aa0",
   "metadata": {},
   "source": [
    "this script:\n",
    "- loads all files specified in {what_to_load}\n",
    "- scores loaded questionnaires specified in {what_to_score} - note only works for those for which myfx_scoring.py has scoring fx for\n",
    "- combines data and scored subscales (dropping item-level questionnaire responses) into a long dataframe (this is 4 times N x Cols)\n",
    "- (optional) transform form long to wide - by creating one column_T1|_T2|_T3|_T4 per subscale (this is N x 4 times the Cols)\n",
    "- save to file\n",
    "\n",
    "Note: need to run this script only once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bc4e3-ccce-4cc4-a164-c5fb4bb73319",
   "metadata": {},
   "source": [
    "## Load and Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4440fb0-ae7f-48a9-adbf-9fbbebacd4ad",
   "metadata": {},
   "source": [
    "### Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3155ed34-2265-44c0-8ab4-ee4d3569b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import functools as ft\n",
    "\n",
    "#custom\n",
    "import myfx_scoring as myfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f0316-58fb-4cb8-a4bf-7371257fc13a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d699cfc-2f84-4e87-aa71-08a84850c8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## paths\n",
    "main_d = '/work/swglab/data/BANDA'\n",
    "behave_d_in = f'{main_d}/NDA/BANDAImgManifestBeh'\n",
    "behave_d_out = f'{main_d}/sourcedata/behavioral_data'\n",
    "\n",
    "## general parameters\n",
    "merge_cols = ['src_subject_id', 'visit','respondent','subjectkey','sex','interview_date','interview_age'] #cols that will be used to merged files\n",
    "custom_cols = ['respondent','interview_age','interview_date'] #cols present in all source files but to keep specific to each file source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b22be81-3061-496a-b678-1b18bb9527a7",
   "metadata": {},
   "source": [
    "## files to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c976cff-b77a-43a6-a245-ed676c8620e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you want to upload all files from {behave_d_in}\n",
    "dont_load = ['dataset_collection', 'datastructure_manifest', 'md5_values', 'package_info', 'fmriresults01','imagingcollection01','fhs01'] #these files have general dataset info , not data per se\n",
    "what_to_load = [f.replace(f'{behave_d_in}/','').replace('.txt','') for f in glob(f'{behave_d_in}/*.txt')] # returns only the name of the files in {behave_d_in} without abs path\n",
    "what_to_load = [f for f in what_to_load if f not in dont_load]\n",
    "\n",
    "## if you want a subset of files\n",
    "# what_to_load = ['mfq01','ksads_diagnoses01'] #these names need to be a substring of the filename in behave_d_in (without extentions). e.g., 'cbcl' works to load 'cbcl01.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d78c99-c36b-49fc-a476-1a718e107e84",
   "metadata": {},
   "source": [
    "## files to score\n",
    "\n",
    "note: can't score something that has not been loaded in {what_to_load} and that's not included in the myfx_scoring.py functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e17bd66-231c-48d7-a4d7-205ffedf0d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to score everything possible (ie use all scoring functions included in the myfx module)\n",
    "what_to_score = [eval(f'myfx.{fx}') for fx in dir(myfx) if 'score_' in fx] #grabbing all functions lik myfx.score_xxx from the scoring file\n",
    "\n",
    "## if you want to specify a subsample of questionnaires\n",
    "# what_to_score = [myfx.score_mfq,\n",
    "#                  myfx.score_rcads]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b425dd4b-8400-4bac-ab7d-7a982c51adaa",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e668296-97b0-4ec0-aa75-c3b66732e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfx_load_behave(behave_d_in, questionnaire, merge_cols):\n",
    "\n",
    "    '''\n",
    "    behave_d_in : string representing an absolute path of parent directory for where file.txt live\n",
    "    questionnaire : file name to load. no need to be exact name but should be included in filename e.g. 'demo' would work for demographic01.txt\n",
    "    merge_cols : list of columns used as index to merge data from different files on\n",
    "    '''    \n",
    "    print(q)\n",
    "\n",
    "    # load\n",
    "    file = glob(f'{behave_d_in}/*{q}*')\n",
    "    assert len(file) == 1, f'watch out: there are multiple files corresponding to {q}. Specify which is needed from these by passing a keyword that is unique:\\n{file}'\n",
    "    file = file[0]\n",
    "    grab = pd.read_csv(f'{file}', sep='\\t', skiprows=[1])\n",
    "\n",
    "    # harmonize info between files\n",
    "    to_fix = ['dccs01','flanker01','lswmt01','orrt01','pcps01','ndar_subject01','ksads_diagnoses01'] #these have no info about Respondent but it was always Child, so hardcoding it here \n",
    "    if q in to_fix:\n",
    "        if 'respondent' not in grab.columns or grab['respondent'].isnull().all():\n",
    "            grab['respondent'] = 'Child'\n",
    "\n",
    "    grab = grab.set_index(merge_cols)\n",
    "\n",
    "    # clean\n",
    "    drop_cols = grab.columns[grab.columns.str.endswith('_id') | grab.columns.str.contains('version')].tolist() + ['collection_title'] # grab columns e.g., ['rcads_id', 'dataset_id', 'flanker01_version_form'] to be dropped\n",
    "    grab = grab.drop(columns = drop_cols)\n",
    "    grab = grab.replace('NaN',np.nan).replace(\"NaN\",np.nan)\n",
    "\n",
    "    if q not in ['pwmt01','pmat01','er4001','deldisk01']: #these may have 999 which are meaningful values so not dropping those, for other columsn 999/99999 etc represent missing - so substituting\n",
    "        grab = grab.replace(999,np.nan).replace(9999,np.nan).replace(9998,np.nan)\n",
    "        \n",
    "    grab = grab.dropna(axis=1, how='all') # drop if every value that column is NaN\n",
    "    grab = grab.loc[:, (grab!=\"None\").all(axis = 0)]\n",
    "    grab = grab.loc[:, (grab!=\"No\").all(axis = 0)]\n",
    "\n",
    "    # adding prefix corresponding to file name so that it'd be easy to track each variable back to each file\n",
    "    # note this operation is reverted inside of each scoring_fx so that variable names included there match with original NDA nomenclature\n",
    "    # ultimately, questionnaires that have been scored will have mycustolabel_subscale_name\n",
    "    # those that are not scored (eg nih toolbox etc) will have origfilename_originalcolname\n",
    "    grab = grab.add_prefix(f'{q}_')\n",
    "\n",
    "    return grab, f'{q}_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a2128-ed2e-458f-8f82-7cf839d9b308",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd5140-fd56-4152-b58b-6f8de1ca0c6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load and combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5812ecda-03d9-4f88-9e9a-3d026b919bdf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bisbas01\n",
      "cbcl01\n",
      "chaphand01\n",
      "cssrs01\n",
      "dccs01\n",
      "deldisk01\n",
      "demographics02\n",
      "er4001\n",
      "flanker01\n",
      "ksads_diagnoses01\n",
      "ksads_diagnosesp201\n",
      "lswmt01\n",
      "masq01\n",
      "mfq01\n",
      "ndar_subject01\n",
      "nffi01\n",
      "orrt01\n",
      "pcps01\n",
      "pmat01\n",
      "pwmt01\n",
      "rbqa01\n",
      "rcads01\n",
      "rmbi01\n",
      "shaps01\n",
      "stai01\n",
      "strain01\n",
      "tanner_sms01\n",
      "wasi201\n",
      "\n",
      "MERGING by ['src_subject_id', 'visit', 'respondent', 'subjectkey', 'sex', 'interview_date', 'interview_age']\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "filename = []\n",
    "for q in what_to_load:\n",
    "    grab, questionnaire = myfx_load_behave(behave_d_in, q, merge_cols)\n",
    "    dfs.append(grab)\n",
    "    filename.append(questionnaire)\n",
    "\n",
    "# combine dataframes        \n",
    "print(f'\\nMERGING by {merge_cols}')\n",
    "df = ft.reduce(lambda left, right: pd.merge(left, right, on=merge_cols, how='outer', validate=\"one_to_one\"), dfs)\n",
    "df = df.replace('NaN',np.nan).replace(\"NaN\",np.nan).sort_values(by= merge_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61707f-6179-4de6-adbe-42dab1734f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Score questionnaires\n",
    "for each questionnaire indicated to be scored in {what_to_score}, this function: \n",
    "- generates scored subscales and total score\n",
    "- returns number of nan in the item-level which would have been used to calculate subscale or totals\n",
    "- return an error if any subscale has values outside the theoretical range assumed by the questionnaire\n",
    "- return an error if there are more or fewer amount of items used to generate the subscales\n",
    "\n",
    "if specified:\n",
    "- removes item-level columns\n",
    "- returns count of all items which should have been used to generate each individual subscale or total (ie item_count - nan_count = number of items used/available)\n",
    "\n",
    "output: one row per respondent {parent|child} and assessment timepoint {T1|T2|T3|T4} and one column per subscale or per original columns found in the loaded files (in cases when a questionnaire was loaded but not scored)\n",
    "\n",
    "note: if you specify to drop original items, you can't run this twice in a row without re-loading original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d4aac9-42e1-4a89-868e-b02f57fb3ee2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored bisbas\n",
      "scored chaphand\n",
      "scored cssrs\n",
      "scored ksads\n",
      "scored ksadsp201\n",
      "Scored masq\n",
      "Scored mfq\n",
      "scored nffi\n",
      "Remaned PENN tasks and NIH toolbox tasks\n",
      "scored rbqa\n",
      "scored rcads\n",
      "scored rmbi\n",
      "scored shaps\n",
      "Scored stai\n",
      "scored tanner\n",
      "scored wasi\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "for grab_fx in what_to_score:\n",
    "    df = grab_fx(df, item_level = 'drop', grab_item_count = 'no') # item_level : 'drop' or ''; grab_item_count : 'yes' or ''\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c3807-6f77-42ba-aab1-449b8a5d9daf",
   "metadata": {},
   "source": [
    "# Reality Checks at the Dataframe level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8d17d3f-aed5-404e-9337-0fc24c835768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: no duplicate indexes found\n",
      "OK: no duplicate assessment dates for respondents\n",
      "OK: no columns have elements that are all NaNs\n",
      "OK: no extra participants: sample size of respondend x timepoint is always < n=215\n",
      "\n",
      "All good\n"
     ]
    }
   ],
   "source": [
    "# check that there are no duplicate cases\n",
    "assert len(df.index[df.index.duplicated(keep=False)].values) == 0, 'Watch out: there are duplicated indexes'\n",
    "print('OK: no duplicate indexes found')\n",
    "\n",
    "# more specific: check that each respondent is included only once (regardless of sex, interview_data, and interview_age)\n",
    "assert len(df.index[df.index.droplevel(['sex','interview_date','interview_age']).duplicated()].values) == 0, 'Watch out: there are duplicated indexes'\n",
    "print('OK: no duplicate assessment dates for respondents')\n",
    "\n",
    "# check that there are no columns with just NaNs for all participants\n",
    "assert df[df.columns[df.isna().all(axis = 0)]].shape[1] == 0, 'Watch out: there are some columns that only have all NaNs' #looking for columns that are all nan - there should not be any\n",
    "print ('OK: no columns have elements that are all NaNs')\n",
    "\n",
    "# check total N by respondent {parent, child} and timepoints x {T1, T2, T3, T4} is < 215 (which is N of recruited participants)\n",
    "for r in ['Parent','Child']:\n",
    "    for t in ['T1','T2','T3','T4']:\n",
    "        n_cases = len(df.loc[:,(f'{t}'),(f'{r}'),:,:,:,:])\n",
    "        assert n_cases <= 215, f'Watch out: there are more indexes corresponding to {r} at {t} than 215'        \n",
    "print('OK: no extra participants: sample size of respondend x timepoint is always < n=215')\n",
    "print('\\nAll good')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e4ad72-a4d4-4999-b22b-5a204b7dc125",
   "metadata": {},
   "source": [
    "### Save scored data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7579a-976d-4835-acf0-3ca999b7c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{behave_d_out}/banda_behave.csv', sep = \",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
